{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from networkx.algorithms.bipartite.basic import color\n",
    "import re\n",
    "from wordcloud import WordCloud"
   ],
   "id": "ea490d5258dc703f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import data of reviewed abstracts:\n",
    "gs_abstracts = pd.read_excel('3_title_and_abstract_screening/scholar_unique.xlsx',nrows=200)\n",
    "scopus_abstracts = pd.read_excel('3_title_and_abstract_screening/scopus_unique.xlsx')\n",
    "wos_abstracts = pd.read_excel('3_title_and_abstract_screening/wos_unique.xlsx')\n",
    "\n",
    "print(gs_abstracts.shape, scopus_abstracts.shape, wos_abstracts.shape)"
   ],
   "id": "97cc08334f418c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove duplicates found in wos_abstracts:\n",
    "wos_abstracts=wos_abstracts[~(wos_abstracts['Reason for inclusion or exclusion']=='Duplicated')]\n",
    "wos_abstracts.shape"
   ],
   "id": "c0526944b6fdf0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the column names\n",
    "col_names = ['Included (0 No, 1 Yes)', 'Reason for inclusion or exclusion', 'Authors', 'Title', 'Year', 'Source', 'ArticleURL', 'DOI']\n",
    "\n",
    "# Select and rename columns for each dataset\n",
    "selected_gs = gs_abstracts[col_names]\n",
    "selected_scopus = scopus_abstracts[['Included (0 No, 1 Yes)', 'Reason for inclusion or exclusion', 'Authors', 'Title', 'Year', 'Source title', 'Link', 'DOI']]\n",
    "selected_wos = wos_abstracts[['Included (0 No, 1 Yes)', 'Reason for inclusion or exclusion', 'Authors', 'Article Title', 'Publication Year', 'Source Title', 'DOI Link', 'DOI']]\n",
    "\n",
    "# Rename columns to match col_names\n",
    "selected_scopus.columns = col_names\n",
    "selected_wos.columns = col_names\n",
    "\n",
    "# Combine the datasets\n",
    "selected_combined = pd.concat([selected_gs, selected_scopus, selected_wos], ignore_index=True, axis='index')\n",
    "selected_combined.shape"
   ],
   "id": "1ad2d3f09953a8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Analyses",
   "id": "3d953d2d469ae69d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Descriptive Statistics\n",
    "\n",
    "* Publication Trends: Count papers per year to see trends over time.\n",
    "* Author Analysis: Identify the most prolific authors in the field.\n",
    "* Journal Distribution: See which journals or conferences publish the most relevant studies."
   ],
   "id": "56e35e8e9e27a751"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Publication Trends\n",
    "# Count papers per year and sort by year\n",
    "year_trend = selected_combined['Year'].value_counts()\n",
    "# Remove 2025 since it is not a valid year\n",
    "year_trend.drop(2025,inplace=True)\n",
    "# Sort the values by year\n",
    "year_trend = year_trend.reset_index().sort_values(by='Year',ignore_index=True)"
   ],
   "id": "7c60956b6a2bc4ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a plotly plot\n",
    "fig_width=800; fig_height=600\n",
    "fig = px.bar(\n",
    "    year_trend,\n",
    "    x='Year',\n",
    "    y='count',\n",
    "    title='Publications per Year',\n",
    "    labels={'Year': 'Publication Year', 'count': 'Number of Publications'},\n",
    "    color_discrete_sequence=['cornflowerblue']\n",
    ")\n",
    "\n",
    "# Update layout for better aesthetics\n",
    "fig.update_layout(\n",
    "    title={'text': 'Publications per Year', 'x': 0.5, 'xanchor': 'center', 'font': {'size': 24, 'family': 'Arial'}},\n",
    "    xaxis_title={'text': 'Year of Publication', 'font': {'size': 18, 'family': 'Arial'}},\n",
    "    yaxis_title={'text': 'Number of Publications', 'font': {'size': 18, 'family': 'Arial'}},\n",
    "    xaxis={\n",
    "        'tickangle': 270, 'tickfont': {'size': 16, 'family': 'Arial'},\n",
    "        'showgrid': False, 'gridcolor': 'lightgrey',\n",
    "        'range': [2013.5, 2024.5],\n",
    "        'dtick': 1\n",
    "    },\n",
    "    yaxis={'tickfont': {'size': 16, 'family': 'Arial'},\n",
    "    'showgrid': True, 'gridcolor': 'lightgrey',\n",
    "    'range': [0, 110]\n",
    "    },\n",
    "    coloraxis_showscale=False,\n",
    "    plot_bgcolor='white',\n",
    "    margin={'l': 60, 'r': 20, 't': 60, 'b': 60},\n",
    "    width=fig_width,\n",
    "    height=fig_height\n",
    ")\n",
    "# Export the plot as png, pdf and svg\n",
    "fig.write_image('3_title_and_abstract_screening/publication_trends.png', format='png')\n",
    "fig.write_image('3_title_and_abstract_screening/publication_trends.pdf', format='pdf')\n",
    "fig.write_image('3_title_and_abstract_screening/publication_trends.svg', format='svg')\n",
    "fig.show()"
   ],
   "id": "51de0052f09afca2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Author Analysis\n",
    "# Count the number of papers per author\n",
    "author_trend = selected_combined['Authors']\n",
    "# Remove missing values\n",
    "author_trend = author_trend.dropna()"
   ],
   "id": "52e687a615642f17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sample data\n",
    "data = author_trend\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to normalize and split author names\n",
    "def normalize_authors(authors):\n",
    "    # Replace semicolons and commas with a common delimiter\n",
    "    authors = re.sub(r'[;,]', ',', authors)\n",
    "    # Remove full stops\n",
    "    authors = authors.replace('.', '')\n",
    "    # Split the authors by the common delimiter\n",
    "    author_list = authors.split(',')\n",
    "    # Strip whitespace and standardize format\n",
    "    standardized_authors = []\n",
    "    for author in author_list:\n",
    "        author = author.strip()\n",
    "        if re.match(r'^[A-Z]{1,2} [A-Z][a-z]+$', author):  # Format: Initial Lastname\n",
    "            parts = author.split(' ')\n",
    "            standardized_authors.append(f'{parts[1]} {parts[0]}')\n",
    "        else:\n",
    "            standardized_authors.append(author)\n",
    "    return standardized_authors\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df['Normalized Authors'] = df['Authors'].apply(normalize_authors)\n",
    "\n",
    "# Flatten the list of authors and get unique names\n",
    "all_authors = [author for sublist in df['Normalized Authors'] for author in sublist]"
   ],
   "id": "27d2f83618a72d1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.Series(all_authors).value_counts()",
   "id": "ab962582bc8794c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Journal Distribution\n",
    "# Count the number of papers per journal\n",
    "journal_trend = selected_combined['Source']\n",
    "journal_trend.value_counts()"
   ],
   "id": "ca717022fe3117d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I guess the Author and Journal analyses are not very useful, since it is hard to distinguish the unique authors (lots of similar last names and hard to get the unique last-name-and-initials pairs) and journals (lots of ellipsis dots...) from the data.b",
   "id": "5f57b390767bcbef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exclusion Analysis\n",
    "* Top Reasons for Exclusion: See if certain recurring issues (e.g., \"no time progression analysis\") explain most rejections.\n"
   ],
   "id": "1f876752800cc598"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "exclusion_reasons =  selected_combined['Reason for inclusion or exclusion'][selected_combined['Included (0 No, 1 Yes)']==0]",
   "id": "8a7080cc89d82095",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Top Reasons for Exclusion\n",
    "exclusion_reasons.value_counts()"
   ],
   "id": "cfee946e149a7d24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sample data\n",
    "data = exclusion_reasons\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Function to tokenize and count words\n",
    "def count_words(text_series):\n",
    "    # Convert to lowercase\n",
    "    text_series = text_series.str.lower()\n",
    "    # Tokenize the text\n",
    "    words = text_series.apply(lambda x: re.findall(r'\\b\\w+\\b', x))\n",
    "    # Flatten the list of words\n",
    "    all_words = [word for sublist in words for word in sublist]\n",
    "    return all_words\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "all_words = count_words(df['Reason for inclusion or exclusion'])\n",
    "\n",
    "word_count=pd.Series(all_words).value_counts()"
   ],
   "id": "b9975d04fd2db6d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove prepositions and conjunctions, and also drop \"unrelated\" since it is not informative. Same for \"mentions\".\n",
    "filtered_words=word_count.drop(labels=['and','of','no','or','not','an','on','from','with','to','unrelated','mentions','mention',]).iloc[:20]\n",
    "# Substitute \"article\" with \"not an article\" since it is used in the negative case.\n",
    "filtered_words=filtered_words.rename(index={'article':'not an article'})\n",
    "# Substitute \"progression\" with \"no progression\" since it is used in the negative case.\n",
    "filtered_words=filtered_words.rename(index={'progression':'no progression'})\n",
    "# Same for time or temporal\n",
    "filtered_words=filtered_words.rename(index={'time':'no time','temporal':'no temporal'})\n",
    "filtered_words"
   ],
   "id": "aa8376f911b35962",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(filtered_words)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig('3_title_and_abstract_screening/exclusion_reasons_wordcloud.png', format='png', bbox_inches='tight', dpi=300)\n",
    "plt.savefig('3_title_and_abstract_screening/exclusion_reasons_wordcloud.pdf', format='pdf', bbox_inches='tight')\n",
    "plt.savefig('3_title_and_abstract_screening/exclusion_reasons_wordcloud.svg', format='svg', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ],
   "id": "d02d2c4eb16a5d1a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
